<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Esam Ghaleb</title> <meta name="author" content="Esam Ghaleb"/> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/nn.png"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://esamghaleb.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1SQB2F51BX"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1SQB2F51BX");</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%65%67%68%61%6C%65%62@%75%76%61.%6E%6C" title="email"><i class="fas fa-envelope"></i></a> <a href="https://orcid.org/0000-0002-0603-9817" title="ORCID" target="_blank" rel="noopener noreferrer"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=TqP9GTsAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/esamghaleb" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/esamghaleb" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/esam__ghaleb" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a> <a href="https://facebook.com/esam.bin.ghaleb" title="Facebook" target="_blank" rel="noopener noreferrer"><i class="fab fa-facebook"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching and Supervision</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Esam Ghaleb </h1> <p class="desc">Affiliation - <a href="https://www.illc.uva.nl" target="_blank" rel="noopener noreferrer">ILLC</a> at <a href="https://www.uva.nl" target="_blank" rel="noopener noreferrer">UvA</a>. Address - LAB42 (room L6.39), Science Park 900, 1012 WX, Amsterdam, The Netherlands.</p> </header> <article> <div class="profile float-right"> <figure> <picture> <img src="/assets/img/EsamProfile2022_edt.JPG" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="EsamProfile2022_edt.JPG" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="clearfix"> <p>Esam Ghaleb is a postdoctoral researcher in the <a href="https://dmg-illc.github.io/dmg/" target="_blank" rel="noopener noreferrer">Dialogue Modelling Group</a> at the Institute for Logic, Language &amp; Computation (ILLC), University of Amsterdam. Currently, he is working on modeling face-to-face dialogue using NLP, computer vision, and machine learning. His work is embedded within the <a href="https://www.languageininteraction.nl/" target="_blank" rel="noopener noreferrer">Language in Interaction</a> consortium.</p> <p>Previously, Ghaleb was a postdoctoral researcher in the Department of Advanced Computing Sciences at Maastricht University, working on the Horizon 2020 project, <a href="https://procare4life.eu/" target="_blank" rel="noopener noreferrer">ProCare4Life</a>. In the ProCare4Life project, his research focused on human behavioral analysis, mainly for people with neurodegenerative diseases. During his postdoctoral research, he worked on XAI for bodily expressed emotion recognition as well. Previously, Ghaleb completed his doctoral research at Maastricht University. During his Ph.D., he carried out research on bimodal emotion recognition through audio-visual cues. His work utilized facial expressions and speech signals in video clips and proposed progressive computational methodologies to capture their complementary information for emotion recognition. Ghalebâ€™s Ph.D. research was supported by the Horizon 2020 project <a href="http://mathisis-project.eu/" target="_blank" rel="noopener noreferrer">MaTHiSiS</a>. Besides his research and project work, he was a teaching assistant for the computer vision course at DACS at Maastricht University.</p> <p>Before joining UM, Ghaleb completed his BA (2013) and MA (2015) degrees in the Department of Computer Engineering at Istanbul Technical University (ITU). In his Masterâ€™s thesis, he studied the aging effect on face recognition utilizing Harry Potter movies. Early academic work experiences include his position as a research assistant in the Smart Interaction, Mobile Intelligence, and Multimedia Technologies (SiMiT) Research Group at Istanbul Technical University (ITU). Furthermore, Ghaleb conducted two internships in the Computer Vision for Human-Computer Interaction (CVHCI) laboratory at Karlsruhe Institute of Technology (2014 &amp; 2015).</p> </div> <div class="news"> <h2>News</h2> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Oct 18, 2022</th> <td> <a class="news-title" href="/news/announcement_1%20copy/">NLPitch at the ILLC!</a> </td> </tr> <tr> <th scope="row">Sep 1, 2022</th> <td> I joined the <a href="https://dmg-illc.github.io/dmg/" target="_blank" rel="noopener noreferrer">Dialogue Modelling Group</a> as a postdoc to work on modelling face-to-face dialogue within the <a href="https://www.languageininteraction.nl/" target="_blank" rel="noopener noreferrer">Language in Interaction</a> consortium! âœ¨ ðŸ˜„ </td> </tr> <tr> <th scope="row">Jul 15, 2022</th> <td> <a class="news-title" href="/news/announcement_2/">New paper published in Multimedia Tools and Applications</a> </td> </tr> <tr> <th scope="row">Nov 15, 2021</th> <td> <a class="news-title" href="/news/announcement_3/">Paper accepted at FG21!</a> </td> </tr> <tr> <th scope="row">Jul 8, 2021</th> <td> <a class="news-title" href="/news/announcement_4/">I defended my PhD thesis!</a> </td> </tr> </table> </div> </div> <div class="publications"> <h2>Selected Publications</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://www.springer.com/journal/11042" target="_blank" rel="noopener noreferrer">MTAP</a></abbr></div> <div id="ghaleb2022joint" class="col-sm-8"> <div class="title">Joint Modelling of Audio-visual Cues Using Attention Mechanism for Emotion Recognition</div> <div class="author"> Esam Ghaleb,Â Jan Niehues,Â andÂ Stylianos Asteriadis</div> <div class="periodical"> <em>Multimedia Tools and Applications</em> Jul 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/article/10.1007/s11042-022-13557-w" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/GhalebJoint.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Emotions play a crucial role in human-human communications with complex socio-psychological nature. In order to enhance emotion communication in human-computer interaction, this paper studies emotion recognition from audio and visual signals in video clips, utilizing facial expressions and vocal utterances. Thereby, the study aims to exploit temporal information of audio-visual cues and detect their informative time segments. Attention mechanisms are used to exploit the importance of each modality over time. We propose a novel framework that consists of bi-modal time windows spanning short video clips labeled with discrete emotions. The framework employs two networks, with each one being dedicated to one modality. As input to a modality-specific network, we consider a time-dependent signal deriving from the embeddings of the video and audio modalities. We employ the encoder part of the Transformer on the visual embeddings and another one on the audio embeddings. The research in this paper introduces detailed studies and meta-analysis findings, linking the outputs of our proposition to research from psychology. Specifically, it presents a framework to understand underlying principles of emotion recognition as functions of three separate setups in terms of modalities: audio only, video only, and the fusion of audio and video. Experimental results on two datasets show that the proposed framework achieves improved accuracy in emotion recognition, compared to state-of-the-art techniques and baseline methods not using attention mechanisms. The proposed method improves the results over baseline methods by at least 5.4%. Our experiments show that attention mechanisms reduce the gap between the entropies of unimodal predictions, which increases the bimodal predictionsâ€™ certainty and, therefore, improves the bimodal recognition rates. Furthermore, evaluations with noisy data in different scenarios are presented during the training and testing processes to check the frameworkâ€™s consistency and the attention mechanismâ€™s behavior. The results demonstrate that attention mechanisms increase the frameworkâ€™s robustness when exposed to similar conditions during the training and the testing phases. Finally, we present comprehensive evaluations of emotion recognition as a function of time. The study shows that the middle time segments of a video clip are essential in the case of using audio modality. However, in the case of video modality, the importance of time windows is distributed equally.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ghaleb2022joint</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Joint Modelling of Audio-visual Cues Using Attention Mechanism for Emotion Recognition}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ghaleb, Esam and Niehues, Jan and Asteriadis, Stylianos}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Multimedia Tools and Applications}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{journal}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1007/s11042-022-13557-w}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://link.springer.com/article/10.1007/s11042-022-13557-w}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="http://iab-rubric.org/fg2021/" target="_blank" rel="noopener noreferrer">FG21</a></abbr></div> <div id="ghalebskeleton" class="col-sm-8"> <div class="title">Skeleton-Based Explainable Bodily Expressed Emotion Recognition Through Graph Convolutional Networks</div> <div class="author"> Esam Ghaleb,Â AndrÃ© Mertens,Â Stylianos Asteriadis, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Gerhard Weiss' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In 2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021)</em> Jul 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/abstract/document/9667052" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/GhalebSkeletonFG21.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Much of the focus on emotion recognition has gone into the face and voice as expressive channels, whereas bodily expressions of emotions are understudied. Moreover, current studies lack the explainability of computational features of body movements related to emotional expressions. Perceptual research on body partsâ€™ movements shows that features related to the armsâ€™ movements are correlated the most with human perception of emotions. In this paper, our research aims at presenting an explainable approach for bodily expressed emotion recognition. It utilizes the body joints of the human skeleton, representing them as a graph, which is used in Graph Convolutional Networks (GCNs). We improve the modelling of the GCNs by using spatial attention mechanisms based on body parts, i.e. arms, legs and torso. Our study presents a state-of-the-art explainable approach supported by experimental results on two challenging datasets. Evaluations show that the proposed methodology offers accurate performance and explainable decisions. The methodology demonstrates which body part contributes the most in its inference, showing the significance of arm movements in emotion recognition.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ghalebskeleton</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Skeleton-Based Explainable Bodily Expressed Emotion Recognition Through Graph Convolutional Networks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ghaleb, Esam and Mertens, Andr{\'e} and Asteriadis, Stylianos and Weiss, Gerhard}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%65%67%68%61%6C%65%62@%75%76%61.%6E%6C" title="email"><i class="fas fa-envelope"></i></a> <a href="https://orcid.org/0000-0002-0603-9817" title="ORCID" target="_blank" rel="noopener noreferrer"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=TqP9GTsAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/esamghaleb" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/esamghaleb" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/esam__ghaleb" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a> <a href="https://facebook.com/esam.bin.ghaleb" title="Facebook" target="_blank" rel="noopener noreferrer"><i class="fab fa-facebook"></i></a> </div> <div class="contact-note"> You can best contact me through email. </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> Â© Copyright 2023 Esam Ghaleb. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1SQB2F51BX"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1SQB2F51BX");</script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1SQB2F51BX"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1SQB2F51BX");</script> </body> </html>