---
layout: about
title: About
permalink: /
nav_order: 1
profile:
  align: right
  image: EsamProfile2022_edt.JPG
  image_circular: false # crops the image to make it circular

news: true  # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true  # includes social icons at the bottom of the page
---
In brief: I develop computational models of multimodal language and behaviours: how speech, text, gesture, sign, facial expressions, and whole-body movement jointly encode meaning in interaction. Previously, I worked at the University of Amsterdam and Maastricht University on linguistic–gestural alignment and explainable multimodal modelling of human behaviour. 

I am research staff in the Multimodal Language Department at the Max Planck Institute for Psycholinguistics (Nijmegen), and I lead the department’s Multimodal Modelling Cluster. My work builds and studies machine-learning methods for the segmentation, coding, and representation of visual communicative signals from motion-capture and video data, and uses learned representations as testbeds for theories of multimodal language across languages and interactional settings. I also study how large language and multimodal language models integrate and generate multimodal behaviour, and develop generative models of gesture for virtual agents. Recent work includes an NWO XS-funded project on grounded, object- and interaction-aware gesture generation in context.

<!-- 

**In brief**: I study and model how verbal and non-verbal cues work together in a variety of human behaviors. Since September 2024 I have been a researcher in the [Multimodal Language Department](https://www.mpi.nl/department/multimodal-language-department/23) at the Max Planck Institute for Psycholinguistics, where I model multimodal communication for both human insight and machine applications.

Trained in computer science and engineering, I work across AI, cognitive science, psycholinguistics, psychology and healthcare to computationally model human behaviour for both fundamental and applied research. My work focuses on multimodal interaction, particularly in the context of dialogue. My research spans various domains, including gesture generation, multimodal dialogue systems, and previously affective computing, with applications in healthcare, human-computer interaction, and social robotics.

During my PhD and post-doc at Maastricht University, I developed explainable multimodal emotion-recognition techniques; at the Institute for Logic, Language & Computation (University of Amsterdam) I investigated linguistic–gestural alignment and automatic gesture segmentation in dialogues. My applied projects include two EU-funded studies (200+ participants) and a work package that combined clinicians’ expertise with machine intelligence for socio-economic contexts. 

Born in Yemen, I ranked in the top 0.2 % of high-school graduates, then earned BSc and MSc degrees (with honours) in computer engineering at Istanbul Technical University, including an internship at Karlsruhe Institute of Technology. An early open-access dataset on age-invariant face recognition launched my interest in multimodal behaviour. -->