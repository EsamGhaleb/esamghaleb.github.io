<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Human Activity Recognition | Esam Ghaleb</title> <meta name="author" content="Esam Ghaleb"/> <meta name="description" content="Self-Suerpvised Learning (SSL) for sensor-based human activity recognition"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/Esam%20Ghaleb.png"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://esamghaleb.github.io/projects/06-HAR_Bulat/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1SQB2F51BX"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1SQB2F51BX");</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Esam Ghaleb</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching and Supervision</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Human Activity Recognition</h1> <p class="post-description">Self-Suerpvised Learning (SSL) for sensor-based human activity recognition</p> </header> <article> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/publication_preview/bulat_dynamic_scaling.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> </div> <p>While working as a post-doctoral researcher at Maastricht University, I was fortunate enough to be the daily supervisor of Bulat Khaertdinov, a Ph.D. candidate at UM. I guided and collaborated with Bulat to work in Self-Supervised Learning (SSL) for sensor-based Human Activity Recognition (HAR). During my Ph.D. research, when working on Deep Metric Learning, I recognized that SSL could improve the learning process, especially for those tasks (e.g., sensor-based HAR) where data collection and annotation are time-consuming and expensive. We conducted two studies, introducing a novel method based on contrastive-based SSL. Our study also addressed the problem of negative pairs in contrastive learning by using dynamic temperature scaling within a contrastive loss function. The extensive evaluations of three widely used open-source datasets have shown that the proposed method achieves state-of-the-art SSL activity recognition task results. Furthermore, it has demonstrated strong potential in semi-supervised and transfer learning by outperforming many counterpart baseline methods.</p> </article> <h2>Related Publications</h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/bulat_contrastive.png"></div> <div id="khaertdinov2021contrastive" class="col-sm-8"> <div class="title">Contrastive Self-supervised Learning for Sensor-based Human Activity Recognition</div> <div class="author"> Bulat Khaertdinov, Esam Ghaleb, and Stylianos Asteriadis</div> <div class="periodical"> <em>In 2021 IEEE International Joint Conference on Biometrics (IJCB)</em> 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/abstract/document/9484410?casa_token=0cMXMw8RGnoAAAAA:VL0WEeexrEwYh2nxnU-9E95Y6yWm__Gr59MJHPxU9zIf-02PXVKcOxiXdHiSxJdO-izOLys" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9484410&amp;casa_token=zmflbKiv4OcAAAAA:4vwvVMQ_t0T-fq8xa3wkzh1qJbxakESVjKlkb-teDnIAwzgCh7ZBFXpl3D0P4OglzooJQqc" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>Deep Learning models, applied to a sensor-based Human Activity Recognition task, usually require vast amounts of annotated time-series data to extract robust features. However, annotating signals coming from wearable sensors can be a tedious and, often, not so intuitive process, that requires specialized tools and predefined scenarios, making it an expensive and time-consuming task. This paper combines one of the most recent advances in Self-Supervised Leaning (SSL), namely a SimCLR framework, with a powerful transformer-based encoder to introduce a Contrastive Self-supervised learning approach to Sensor-based Human Activity Recognition (CSSHAR) that learns feature representations from unlabeled sensory data. Extensive experiments conducted on three widely used public datasets have shown that the proposed method outperforms recent SSL models. Moreover, CSSHAR is capable of extracting more robust features than the identical supervised transformer when transferring knowledge from one dataset to another as well as when very limited amounts of annotated data are available.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">khaertdinov2021contrastive</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Contrastive Self-supervised Learning for Sensor-based Human Activity Recognition}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Khaertdinov, Bulat and Ghaleb, Esam and Asteriadis, Stylianos}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2021 IEEE International Joint Conference on Biometrics (IJCB)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--8 (\textbf{second runner up award})}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/triplet_networks.png"></div> <div id="khaertdinov2021deep" class="col-sm-8"> <div class="title">Deep Triplet Networks with Attention for Sensor-based Human Activity Recognition</div> <div class="author"> Bulat Khaertdinov, Esam Ghaleb, and  others</div> <div class="periodical"> <em>In 2021 IEEE International Conference on Pervasive Computing and Communications (PerCom)</em> Mar 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/9439116" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9439116&amp;casa_token=kBJKZlzGieMAAAAA:flEE6bLPPhO6WYiKQX6MYhYH5gTrujzBv6AQsovdTyyJRXryazlHrer427ujRteEQVS7sts" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>One of the most significant challenges in Human Activity Recognition using wearable devices is inter-class similarities and subject heterogeneity. These problems lead to the difficulties in constructing robust feature representations that might negatively affect the quality of recognition. This study, for the first time, applies deep triplet networks with various triplet loss functions and mining methods to the Human Activity Recognition task. Moreover, we introduce a novel method for constructing hard triplets by exploiting similarities between subjects performing the same activities using the concept of Hierarchical Triplet Loss. Our deep triplet models are based on the recent state-of-the-art LSTM networks with two attention mechanisms. The extensive experiments conducted in this paper identify important hyperparameters and settings for training deep metric learning models on widely-used open-source Human Activity Recognition datasets. The comparison of the proposed models against the recent benchmark models shows that deep metric learning approach has the potential to improve the quality of recognition. Specifically, at least one of the implemented triplet networks shows the state-of-the-art results for each dataset used in this study, namely PAMAP2, USC-HAD and MHEALTH. Another positive effect of applying deep triplet networks and especially the proposed sampling algorithm is that feature representations are less affected by inter-class similarities and subject heterogeneity issues.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">khaertdinov2021deep</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Deep Triplet Networks with Attention for Sensor-based Human Activity Recognition}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Khaertdinov, Bulat and Ghaleb, Esam and others}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2021 IEEE International Conference on Pervasive Computing and Communications (PerCom)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--10}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/fg2020.png"></div> <div id="dotti2020temporal" class="col-sm-8"> <div class="title">Temporal triplet mining for personality recognition</div> <div class="author"> Dario Dotti, Esam Ghaleb, and Stylianos Asteriadis</div> <div class="periodical"> <em>In 2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)</em> Mar 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/9320247" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9320247&amp;casa_token=2EwL4EcF1aAAAAAA:LcOyT6B4iGGJtMu2Bqhbs2jtww2Ch82OQMxbQPhSiB93bbrdTqmAg7hj70xOV83HZSB67pw" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>One of the primary goals of personality computing is to enhance the automatic understanding of human behavior, making use of various sensing technologies. Recent studies have started to correlate personality patterns described by psychologists with data findings, however, given the subtle delineations of human behaviors, results are specific to predefined contexts. In this paper, we propose a framework for automatic personality recognition that is able to embed different behavioral dynamics evoked by diverse real world scenarios. Specifically, motion features are designed to encode local motion dynamics from the human body, and interpersonal distance (proxemics) features are designed to encode global dynamics in the scene. By using a Convolutional Neural Network (CNN) architecture which utilizes a triplet loss deep metric learning, we learn temporal, as well as discriminative spatio-temporal streams of embeddings to represent patterns of personality behaviors. We experimentally show that the proposed Temporal Triplet Mining strategy leverages the similarity between temporally related samples and, therefore, helps to encode higher semantic movements or sub-movements which are easier to map onto personality labels. Our experiments show that the generated embeddings improve the state-of-the-art results of personality recognition on two public datasets, recorded in different scenarios.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">dotti2020temporal</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Temporal triplet mining for personality recognition}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Dotti, Dario and Ghaleb, Esam and Asteriadis, Stylianos}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{379--386}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/ieee-tbbis.png"></div> <div id="khaertdinov2022dynamic" class="col-sm-8"> <div class="title">Dynamic Temperature Scaling in Contrastive Self-supervised Learning for Sensor-based Human Activity Recognition</div> <div class="author"> Bulat Khaertdinov, Stylianos Asteriadis, and Esam Ghaleb</div> <div class="periodical"> <em>IEEE Transactions on Biometrics, Behavior, and Identity Science</em> Mar 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/abstract/document/9790823" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9790823&amp;casa_token=hlvooaNVhawAAAAA:Yu26IaVPP6fwF_7y5jujMzoGQKe0xqK2gJOyTLVaFVhzH5cjF4fxMdSbg0hVQ-UqQMCymCI" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>The use of deep neural networks in sensor-based Human Activity Recognition has led to considerably improved recognition rates in comparison to more traditional techniques. Nonetheless, these improvements usually rely on collecting and annotating massive amounts of sensor data, a time-consuming and expensive task. In this paper, inspired by the impressive performance of Contrastive Learning approaches in Self-Supervised Learning settings, we introduce a novel method based on the SimCLR framework and a Transformer-like model. The proposed algorithm addresses the problem of negative pairs in SimCLR by using dynamic temperature scaling within a contrastive loss function. While the original SimCLR framework scales similarities between features of the augmented views by a constant temperature parameter, our method dynamically computes temperature values for scaling. Dynamic temperature is based on instance-level similarity values extracted by an additional model pre-trained on initial instances beforehand. The proposed approach demonstrates state-of-the-art performance on three widely used datasets in sensor-based HAR, namely MobiAct, UCI-HAR and USC-HAD. Moreover, it is more robust than the identical supervised models and models trained with constant temperature in semi-supervised and transfer learning scenarios.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">khaertdinov2022dynamic</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Dynamic Temperature Scaling in Contrastive Self-supervised Learning for Sensor-based Human Activity Recognition}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Khaertdinov, Bulat and Asteriadis, Stylianos and Ghaleb, Esam}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Biometrics, Behavior, and Identity Science}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{journal}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TBIOM.2022.3180591}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/document/9790823}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Esam Ghaleb. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1SQB2F51BX"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1SQB2F51BX");</script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1SQB2F51BX"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1SQB2F51BX");</script> </body> </html>